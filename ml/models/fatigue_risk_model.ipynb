{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577bff11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prepare_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è Warning: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskl2onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not installed. Run \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install skl2onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to enable ONNX export.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# --- Project imports ---\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprepare_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_full_dataset\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model, save_json, compute_metrics, print_metrics\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prepare_data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- ML imports ---\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- ONNX export (optional) ---\n",
    "try:\n",
    "    from skl2onnx import convert_sklearn\n",
    "    from skl2onnx.common.data_types import FloatTensorType\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Warning: 'skl2onnx' not installed. Run 'pip install skl2onnx' to enable ONNX export.\")\n",
    "\n",
    "# --- Project imports ---\n",
    "from prepare_data import prepare_full_dataset\n",
    "from utils import save_model, save_json, compute_metrics, print_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff093f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _zscore(series: pd.Series, window=7, min_periods=3) -> pd.Series:\n",
    "    \"\"\"Compute rolling z-score.\"\"\"\n",
    "    mean = series.rolling(window, min_periods=min_periods).mean()\n",
    "    std = series.rolling(window, min_periods=min_periods).std()\n",
    "    std = std.replace(0, np.nan)\n",
    "    return (series - mean) / std\n",
    "\n",
    "\n",
    "def _delta_from_mean(series: pd.Series, window=7, min_periods=3) -> pd.Series:\n",
    "    \"\"\"Compute deviation from rolling mean.\"\"\"\n",
    "    mean = series.rolling(window, min_periods=min_periods).mean()\n",
    "    return series - mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02415a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fatigue_model_dataset(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load raw health data and create engineered features + fatigue target label.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading dataset from {csv_path} ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: File not found at {csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Standardize column names\n",
    "    df = df.rename(columns={\n",
    "        'participant_id': 'user_id',\n",
    "        'daily_steps': 'steps_total',\n",
    "        'hours_sleep': 'sleep_hours_total',\n",
    "        'calories_burned': 'calories_total'\n",
    "    })\n",
    "\n",
    "    # Ensure consistent format\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['gender_numeric'] = df['gender'].map({'M': 1, 'F': 0})\n",
    "    df = df.sort_values(['user_id', 'date'])\n",
    "\n",
    "    print(\"Calculating rolling z-scores and deltas...\")\n",
    "    df['z_sleep_7d'] = df.groupby('user_id', group_keys=False)['sleep_hours_total'].apply(_zscore)\n",
    "    df['z_steps_7d'] = df.groupby('user_id', group_keys=False)['steps_total'].apply(_zscore)\n",
    "    df['d_sleep_7d'] = df.groupby('user_id', group_keys=False)['sleep_hours_total'].apply(_delta_from_mean)\n",
    "    df['d_steps_7d'] = df.groupby('user_id', group_keys=False)['steps_total'].apply(_delta_from_mean)\n",
    "\n",
    "    # Clean missing values\n",
    "    features_to_check = ['z_sleep_7d', 'z_steps_7d', 'd_sleep_7d', 'd_steps_7d']\n",
    "    before = len(df)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_clean = df.dropna(subset=features_to_check)\n",
    "    print(f\"Removed {before - len(df_clean)} rows with NaNs (initial z-score warm-up).\")\n",
    "\n",
    "    # Create binary fatigue target\n",
    "    df_clean['y_target_fatigue'] = (\n",
    "        (df_clean['z_sleep_7d'] < -1.0) |   # Poor sleep\n",
    "        (df_clean['z_steps_7d'] > 1.5)      # Overactive day\n",
    "    ).astype(int)\n",
    "\n",
    "    print(\"\\n--- Fatigue label distribution ---\")\n",
    "    print(df_clean['y_target_fatigue'].value_counts(normalize=True))\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    # Final dataset columns\n",
    "    cols = [\n",
    "        'user_id', 'date', 'y_target_fatigue',\n",
    "        'steps_total', 'calories_total', 'sleep_hours_total',\n",
    "        'age', 'gender_numeric', 'height_cm', 'weight_kg',\n",
    "        'z_sleep_7d', 'z_steps_7d', 'd_sleep_7d', 'd_steps_7d'\n",
    "    ]\n",
    "    cols = [c for c in cols if c in df_clean.columns]\n",
    "\n",
    "    df_final = df_clean[cols].copy()\n",
    "    print(f\"‚úÖ Dataset ready: {len(df_final)} rows, {len(cols)} columns\")\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fatigue_model(csv_path=None):\n",
    "    \"\"\"\n",
    "    Train the fatigue prediction model, evaluate, and export results.\n",
    "    \"\"\"\n",
    "    # Paths\n",
    "    if csv_path is None:\n",
    "        csv_path = \"/Users/elinakarimova/health-monitoring-app/ml/health_fitness_dataset.csv\"\n",
    "\n",
    "    export_dir = Path(\"/Users/elinakarimova/health-monitoring-app/ml/export\")\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "    onnx_path = export_dir / \"fatigue_model_v1.onnx\"\n",
    "    features_json = export_dir / \"fatigue_model_v1_features.json\"\n",
    "    metrics_json = export_dir / \"fatigue_model_v1_metrics.json\"\n",
    "\n",
    "    print(\"=== Step 1: Data preparation ===\")\n",
    "    df_ready = create_fatigue_model_dataset(csv_path)\n",
    "    if df_ready.empty:\n",
    "        print(\"‚ùå Dataset is empty. Cannot continue.\")\n",
    "        return None, {}\n",
    "\n",
    "    print(\"\\n=== Step 2: Define X, y, groups ===\")\n",
    "    FEATURES = [\n",
    "        'steps_total', 'calories_total', 'sleep_hours_total',\n",
    "        'age', 'gender_numeric', 'height_cm', 'weight_kg',\n",
    "        'd_sleep_7d', 'd_steps_7d'\n",
    "    ]\n",
    "    X = df_ready[FEATURES].fillna(0.0).values\n",
    "    y = df_ready['y_target_fatigue'].values\n",
    "    groups = df_ready['user_id'].values\n",
    "    print(f\"Features ready: {len(FEATURES)} total\")\n",
    "\n",
    "    print(\"\\n=== Step 3: Train/test split ===\")\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "\n",
    "    print(\"\\n=== Step 4: Model training ===\")\n",
    "    models = {}\n",
    "\n",
    "    # --- Logistic Regression ---\n",
    "    logreg = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42))\n",
    "    ])\n",
    "    logreg.fit(X_train, y_train)\n",
    "    proba_lr = logreg.predict_proba(X_test)[:, 1]\n",
    "    pred_lr = (proba_lr >= 0.5).astype(int)\n",
    "    models[\"logreg\"] = {\n",
    "        \"model\": logreg,\n",
    "        \"metrics\": compute_metrics(y_test, pred_lr, proba_lr)\n",
    "    }\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100, min_samples_leaf=5,\n",
    "        random_state=42, n_jobs=-1, class_weight=\"balanced_subsample\"\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "    pred_rf = (proba_rf >= 0.5).astype(int)\n",
    "    models[\"rf\"] = {\n",
    "        \"model\": rf,\n",
    "        \"metrics\": compute_metrics(y_test, pred_rf, proba_rf)\n",
    "    }\n",
    "\n",
    "    # --- Select best model by AUC ---\n",
    "    best_name = max(models, key=lambda k: models[k][\"metrics\"][\"auc\"])\n",
    "    best = models[best_name]\n",
    "    print(f\"\\nüéØ Best model: {best_name.upper()}\")\n",
    "    print_metrics(best[\"metrics\"])\n",
    "\n",
    "    print(\"\\n=== Step 5: Export ===\")\n",
    "    try:\n",
    "        initial_type = [('input', FloatTensorType([None, len(FEATURES)]))]\n",
    "        onx = convert_sklearn(best[\"model\"], initial_types=initial_type, options={'zipmap': False})\n",
    "        with open(onnx_path, \"wb\") as f:\n",
    "            f.write(onx.SerializeToString())\n",
    "        print(f\"‚úÖ Model exported to {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ONNX export failed: {e}\")\n",
    "\n",
    "    # Save metadata\n",
    "    save_json({\"features\": FEATURES}, features_json)\n",
    "    save_json(best[\"metrics\"], metrics_json)\n",
    "\n",
    "    # Also export model in .pkl format for backend\n",
    "    save_model(best[\"model\"], export_dir / \"fatigue_model_v1.pkl\")\n",
    "\n",
    "    print(\"‚úÖ Export complete.\")\n",
    "    return best[\"model\"], best[\"metrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, metrics = train_fatigue_model()\n",
    "    print(\"\\n‚úÖ Model training finished successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
