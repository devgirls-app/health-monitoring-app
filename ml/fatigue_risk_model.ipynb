{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450b998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ –æ—á–∏—Å—Ç–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö... ===\n",
      "--- –ó–∞–≥—Ä—É–∑–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ health_fitness_dataset.csv ---\n",
      "–†–∞—Å—á–µ—Ç Z-scores –∏ Deltas –¥–ª—è —Å–Ω–∞ –∏ —à–∞–≥–æ–≤...\n",
      "–£–¥–∞–ª–µ–Ω–æ 6005 —Å—Ç—Ä–æ–∫ (–ø–µ—Ä–∏–æ–¥ '–ø—Ä–æ–≥—Ä–µ–≤–∞' Z-–æ—Ü–µ–Ω–æ–∫).\n",
      "\n",
      "--- –ë–∞–ª–∞–Ω—Å –Ω–æ–≤–æ–π —Ü–µ–ª–∏ 'y_target_fatigue' ---\n",
      "y_target_fatigue\n",
      "0    0.796713\n",
      "1    0.203287\n",
      "Name: proportion, dtype: float64\n",
      "-------------------------------------------------\n",
      "--- –û—á–∏—Å—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏—á –∑–∞–≤–µ—Ä—à–µ–Ω—ã ---\n",
      "\n",
      "=== –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ X, y, –∏ groups... ===\n",
      "--- –®–∞–≥ 2: X, y, groups —Å–æ–∑–¥–∞–Ω—ã. 9 —Ñ–∏—á ---\n",
      "\n",
      "=== –®–∞–≥ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ 3000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π... ===\n",
      "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: 545520 train, 136176 test.\n",
      "\n",
      "=== –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π... ===\n",
      "–û–±—É—á–µ–Ω–∏–µ 1/2: Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/ngrwl821509g3d0y7t70z0300000gn/T/ipykernel_931/4178425072.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['y_target_fatigue'] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ 2/2: Random Forest...\n",
      "--- –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. ---\n",
      "\n",
      "=== –®–∞–≥ 5: –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===\n",
      "üéâ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: RF\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
      "{\n",
      "  \"acc\": 0.9020899424274468,\n",
      "  \"f1\": 0.7747994257241787,\n",
      "  \"auc\": 0.9592008232426553,\n",
      "  \"precision\": 0.7270191454291872,\n",
      "  \"recall\": 0.8293018042448567\n",
      "}\n",
      "\n",
      "–≠–∫—Å–ø–æ—Ä—Ç 'rf' –≤ ONNX...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: /Users/aruuketurgunbaeva/health-monitoring-app/ml/export/fatigue_model_v1.onnx\n",
      "‚úÖ –ü–æ—Ä—è–¥–æ–∫ —Ñ–∏—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω: /Users/aruuketurgunbaeva/health-monitoring-app/ml/export/fatigue_model_v1_features.json\n",
      "‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: /Users/aruuketurgunbaeva/health-monitoring-app/ml/export/fatigue_model_v1_metrics.json\n",
      "\n",
      "--- –í–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω! ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –ú–æ–¥–µ–ª–µ–π ---\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –≠–∫—Å–ø–æ—Ä—Ç–∞ ---\n",
    "try:\n",
    "    import skl2onnx\n",
    "    from skl2onnx import convert_sklearn\n",
    "    from skl2onnx.common.data_types import FloatTensorType\n",
    "except ImportError:\n",
    "    print(\"–í–Ω–∏–º–∞–Ω–∏–µ: 'skl2onnx' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç.\")\n",
    "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install skl2onnx\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# --- –®–∞–≥ 1: –§—É–Ω–∫—Ü–∏–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö ---\n",
    "# ===================================================================\n",
    "\n",
    "def _zscore(s: pd.Series, window=7, min_periods=3) -> pd.Series:\n",
    "    \"\"\"–†–∞—Å—á–µ—Ç Z-–æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—è std=0.\"\"\"\n",
    "    m = s.rolling(window, min_periods=min_periods).mean()\n",
    "    sd = s.rolling(window, min_periods=min_periods).std()\n",
    "    sd = sd.replace(0, np.nan) \n",
    "    return (s - m) / sd\n",
    "\n",
    "def _delta_from_mean(s: pd.Series, window=7, min_periods=3) -> pd.Series:\n",
    "    \"\"\"–†–∞—Å—á–µ—Ç –¥–µ–ª—å—Ç—ã –æ—Ç —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ.\"\"\"\n",
    "    m = s.rolling(window, min_periods=min_periods).mean()\n",
    "    return s - m\n",
    "\n",
    "def create_fatigue_model_dataset(health_fitness_dataset: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç, –æ—á–∏—â–∞–µ—Ç –µ–≥–æ, –ò–ì–ù–û–†–ò–†–£–Ø –ü–£–õ–¨–°,\n",
    "    –∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é, –∏–Ω–∂–µ–Ω–µ—Ä–Ω—É—é —Ü–µ–ª—å 'y_target_fatigue'.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"--- –ó–∞–≥—Ä—É–∑–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {health_fitness_dataset} ---\")\n",
    "    try:\n",
    "        # --- –ò–°–ü–†–ê–í–õ–ï–ù–û: –û—à–∏–±–∫–∞ 1 (—É–±—Ä–∞–Ω–æ .csv) ---\n",
    "        df = pd.read_csv(health_fitness_dataset)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"–û–®–ò–ë–ö–ê: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {health_fitness_dataset}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- 1.1: –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö –Ω–∞–º —Ñ–∏—á) ---\n",
    "    df = df.rename(columns={\n",
    "        'participant_id': 'user_id',\n",
    "        'daily_steps': 'steps_total',\n",
    "        'hours_sleep': 'sleep_hours_total',\n",
    "        'calories_burned': 'calories_total',\n",
    "        # –ú—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –ò–ì–ù–û–†–ò–†–£–ï–ú 'avg_heart_rate'\n",
    "    })\n",
    "\n",
    "    # --- 1.2: –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Ç–∏–ø—ã, gender) ---\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['gender_numeric'] = df['gender'].map({'M': 1, 'F': 0})\n",
    "\n",
    "    # --- 1.3: –†–∞—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö —Ñ–∏—á (–ë–ï–ó –ü–£–õ–¨–°–ê) ---\n",
    "    print(\"–†–∞—Å—á–µ—Ç Z-scores –∏ Deltas –¥–ª—è —Å–Ω–∞ –∏ —à–∞–≥–æ–≤...\")\n",
    "    df = df.sort_values(['user_id', 'date'])\n",
    "\n",
    "    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Z-–æ—Ü–µ–Ω–∫–∏ –∏ –î–µ–ª—å—Ç—ã –∑–∞ 7 –¥–Ω–µ–π\n",
    "    df['z_sleep_7d'] = df.groupby('user_id', group_keys=False)['sleep_hours_total'].apply(_zscore)\n",
    "    df['z_steps_7d'] = df.groupby('user_id', group_keys=False)['steps_total'].apply(_zscore)\n",
    "    df['d_sleep_7d'] = df.groupby('user_id', group_keys=False)['sleep_hours_total'].apply(_delta_from_mean)\n",
    "    df['d_steps_7d'] = df.groupby('user_id', group_keys=False)['steps_total'].apply(_delta_from_mean)\n",
    "    \n",
    "    # --- 1.4: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–ß–ò–°–¢–ö–ê (NaN) ---\n",
    "    # –ú—ã –¥–æ–ª–∂–Ω—ã —É–¥–∞–ª–∏—Ç—å NaN –î–û —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–∏, —á—Ç–æ–±—ã Z-–æ—Ü–µ–Ω–∫–∏ –±—ã–ª–∏ —á–∏—Å—Ç—ã–º–∏\n",
    "    features_to_check_na = ['z_sleep_7d', 'z_steps_7d', 'd_sleep_7d', 'd_steps_7d']\n",
    "    start_len = len(df)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_cleaned = df.dropna(subset=features_to_check_na)\n",
    "    print(f\"–£–¥–∞–ª–µ–Ω–æ {start_len - len(df_cleaned)} —Å—Ç—Ä–æ–∫ (–ø–µ—Ä–∏–æ–¥ '–ø—Ä–æ–≥—Ä–µ–≤–∞' Z-–æ—Ü–µ–Ω–æ–∫).\")\n",
    "\n",
    "    # --- 1.5: –°–æ–∑–¥–∞–Ω–∏–µ –ù–û–í–û–ô —Ü–µ–ª–∏ 'y_target_fatigue' (–ò–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥) ---\n",
    "    # –ú—ã –ò–ì–ù–û–†–ò–†–£–ï–ú 'stress_level'. –ú—ã —Å–æ–∑–¥–∞–µ–º –°–í–û–Æ —Ü–µ–ª—å \"–£—Å—Ç–∞–ª–æ—Å—Ç—å\".\n",
    "    \n",
    "    # –ù–ê–®–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï \"–£–°–¢–ê–õ–û–°–¢–ò\" (1):\n",
    "    # –î–µ–Ω—å, –∫–æ–≥–¥–∞ —Å–æ–Ω –±—ã–ª –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–û –ù–ò–ñ–ï –Ω–æ—Ä–º—ã (z < -1.0)\n",
    "    # –ò–õ–ò –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±—ã–ª–∞ –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–û –í–´–®–ï –Ω–æ—Ä–º—ã (z > 1.5)\n",
    "    \n",
    "    df_cleaned['y_target_fatigue'] = (\n",
    "        (df_cleaned['z_sleep_7d'] < -1.0) |  # –°–æ–Ω –Ω–∞ 1 std –Ω–∏–∂–µ –Ω–æ—Ä–º—ã\n",
    "        (df_cleaned['z_steps_7d'] >  1.5)    # –®–∞–≥–∏ –Ω–∞ 1.5 std –≤—ã—à–µ –Ω–æ—Ä–º—ã\n",
    "    ).astype(int)\n",
    "\n",
    "    print(\"\\n--- –ë–∞–ª–∞–Ω—Å –Ω–æ–≤–æ–π —Ü–µ–ª–∏ 'y_target_fatigue' ---\")\n",
    "    print(df_cleaned['y_target_fatigue'].value_counts(normalize=True))\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    # --- 1.6: –í—ã–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ ---\n",
    "    FINAL_COLS_NEEDED = [\n",
    "        'user_id',        # –î–ª—è GroupShuffleSplit\n",
    "        'y_target_fatigue', # –ù–∞—à–∞ –ù–û–í–ê–Ø —Ü–µ–ª—å\n",
    "        \n",
    "        # –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —É –Ω–∞—Å —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞:\n",
    "        'steps_total', \n",
    "        'calories_total', \n",
    "        'sleep_hours_total',\n",
    "        'age', \n",
    "        'gender_numeric', \n",
    "        'height_cm', \n",
    "        'weight_kg',\n",
    "        \n",
    "        # –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–∞—Å—Å—á–∏—Ç–∞–ª–∏:\n",
    "        'z_sleep_7d',\n",
    "        'z_steps_7d',\n",
    "        'd_sleep_7d',\n",
    "        'd_steps_7d'\n",
    "    ]\n",
    "    \n",
    "    cols_exist = [col for col in FINAL_COLS_NEEDED if col in df_cleaned.columns]\n",
    "    df_final = df_cleaned[cols_exist].copy()\n",
    "    \n",
    "    print(\"--- –û—á–∏—Å—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏—á –∑–∞–≤–µ—Ä—à–µ–Ω—ã ---\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# --- –û–°–ù–û–í–ù–û–ô –°–ö–†–ò–ü–¢ (–í–´–ü–û–õ–ù–ï–ù–ò–ï) ---\n",
    "# ===================================================================\n",
    "\n",
    "# === –®–∞–≥ 0: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===\n",
    "\n",
    "# --- –ò–°–ü–†–ê–í–¨–¢–ï –≠–¢–£ –°–¢–†–û–ö–£ (–û—à–∏–±–∫–∞ 2) ---\n",
    "# –£–∫–∞–∂–∏—Ç–µ –ü–û–õ–ù–´–ô –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ª–µ–∂–∏—Ç –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ\n",
    "CSV_PATH = \"health_fitness_dataset.csv\"  \n",
    "# –ù–∞–ø—Ä–∏–º–µ—Ä: \"C:\\\\Users\\\\User\\\\Downloads\\\\health_fitness_dataset.csv\"\n",
    "# –ò–ª–∏: \"/Users/User/Downloads/health_fitness_dataset.csv\"\n",
    "\n",
    "EXPORT_DIR = Path(\"export\")\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "ONNX_PATH = EXPORT_DIR / \"fatigue_model_v1.onnx\"\n",
    "FEATURES_JSON = EXPORT_DIR / \"fatigue_model_v1_features.json\"\n",
    "METRICS_JSON = EXPORT_DIR / \"fatigue_model_v1_metrics.json\"\n",
    "\n",
    "\n",
    "# === –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö ===\n",
    "print(\"=== –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ –æ—á–∏—Å—Ç–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö... ===\")\n",
    "df_ready = create_fatigue_model_dataset(CSV_PATH)\n",
    "\n",
    "\n",
    "# === –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ X, y, groups ===\n",
    "FEATURES = [] \n",
    "if not df_ready.empty:\n",
    "    print(\"\\n=== –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ X, y, –∏ groups... ===\")\n",
    "    \n",
    "    # –ù–∞—à –ù–û–í–´–ô —Å–ø–∏—Å–æ–∫ —Ñ–∏—á (–ë–ï–ó –ü–£–õ–¨–°–ê)\n",
    "    FEATURES = [\n",
    "        # –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        'steps_total', \n",
    "        'calories_total', \n",
    "        'sleep_hours_total',\n",
    "        # –°—Ç–∞—Ç–∏–∫–∞\n",
    "        'age', \n",
    "        'gender_numeric', \n",
    "        'height_cm', \n",
    "        'weight_kg',\n",
    "        # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (–î–µ–ª—å—Ç—ã - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –æ–Ω–∏ –Ω–µ –ø—Ä—è–º–∞—è —É—Ç–µ—á–∫–∞)\n",
    "        'd_sleep_7d',\n",
    "        'd_steps_7d'\n",
    "    ]\n",
    "\n",
    "    # .fillna(0.0) - –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è (age, gender, height...)\n",
    "    X = df_ready[FEATURES].fillna(0.0).values\n",
    "    y = df_ready['y_target_fatigue'].values # –ù–∞—à–∞ –Ω–æ–≤–∞—è —Ü–µ–ª—å\n",
    "    groups = df_ready['user_id'].values\n",
    "\n",
    "    print(f\"--- –®–∞–≥ 2: X, y, groups —Å–æ–∑–¥–∞–Ω—ã. {len(FEATURES)} —Ñ–∏—á ---\")\n",
    "else:\n",
    "    print(\"\\n–û–®–ò–ë–ö–ê: 'df_ready' –ø—É—Å—Ç. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.\")\n",
    "\n",
    "\n",
    "# === –®–∞–≥ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Train/Test (–ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º) ===\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    print(f\"\\n=== –®–∞–≥ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ {len(np.unique(groups))} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π... ===\")\n",
    "    \n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "    train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(f\"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(X_train)} train, {len(X_test)} test.\")\n",
    "else:\n",
    "    print(\"\\n–û–®–ò–ë–ö–ê: X –Ω–µ —Å–æ–∑–¥–∞–Ω. –ù–µ –º–æ–≥—É —Ä–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.\")\n",
    "\n",
    "\n",
    "# === –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ===\n",
    "if 'X_train' in locals() and len(X_train) > 0:\n",
    "    print(\"\\n=== –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π... ===\")\n",
    "    models = {}\n",
    "\n",
    "    # --- 4.1 Logistic Regression (Baseline) ---\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ 1/2: Logistic Regression...\")\n",
    "    logreg = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight='balanced', # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'balanced', —Ç.–∫. —Ü–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å (70/30)\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    logreg.fit(X_train, y_train)\n",
    "    proba_lr = logreg.predict_proba(X_test)[:, 1]\n",
    "    pred_lr  = (proba_lr >= 0.5).astype(int)\n",
    "    models['logreg'] = {\n",
    "        \"model\": logreg,\n",
    "        \"metrics\": {\n",
    "            \"acc\": accuracy_score(y_test, pred_lr),\n",
    "            \"f1\": f1_score(y_test, pred_lr),\n",
    "            \"auc\": roc_auc_score(y_test, proba_lr),\n",
    "            \"precision\": precision_score(y_test, pred_lr),\n",
    "            \"recall\": recall_score(y_test, pred_lr),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # --- 4.2 Random Forest (Main Model) ---\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ 2/2: Random Forest...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300, # 300 - —Ö–æ—Ä–æ—à–∏–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=5, # 5 - –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±–æ–±—â–µ–Ω–∏—è\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced_subsample' \n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "    pred_rf  = (proba_rf >= 0.5).astype(int)\n",
    "    models['rf'] = {\n",
    "        \"model\": rf,\n",
    "        \"metrics\": {\n",
    "            \"acc\": accuracy_score(y_test, pred_rf),\n",
    "            \"f1\": f1_score(y_test, pred_rf),\n",
    "            \"auc\": roc_auc_score(y_test, proba_rf),\n",
    "            \"precision\": precision_score(y_test, pred_rf),\n",
    "            \"recall\": recall_score(y_test, pred_rf),\n",
    "        }\n",
    "    }\n",
    "    print(\"--- –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. ---\")\n",
    "else:\n",
    "    print(\"\\n–û–®–ò–ë–ö–ê: X_train –Ω–µ –Ω–∞–π–¥–µ–Ω. –û–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.\")\n",
    "\n",
    "\n",
    "# === –®–∞–≥ 5: –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏ —ç–∫—Å–ø–æ—Ä—Ç ===\n",
    "if 'models' in locals() and 'FEATURES' in locals():\n",
    "    \n",
    "    # --- 5.1 –í—ã–±–æ—Ä –ø–æ–±–µ–¥–∏—Ç–µ–ª—è ---\n",
    "    best_name = max(models.keys(), key=lambda k: models[k][\"metrics\"][\"auc\"])\n",
    "    best_model_data = models[best_name]\n",
    "\n",
    "    print(\"\\n=== –®–∞–≥ 5: –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===\")\n",
    "    print(f\"üéâ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_name.upper()}\")\n",
    "    print(\"–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "    print(json.dumps(best_model_data[\"metrics\"], indent=2))\n",
    "\n",
    "    # --- 5.2 –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX ---\n",
    "    print(f\"\\n–≠–∫—Å–ø–æ—Ä—Ç '{best_name}' –≤ ONNX...\")\n",
    "    try:\n",
    "        initial_type = [('input', FloatTensorType([None, len(FEATURES)]))]\n",
    "        onx = convert_sklearn(\n",
    "            best_model_data[\"model\"], \n",
    "            initial_types=initial_type\n",
    "        )\n",
    "        with open(ONNX_PATH, \"wb\") as f:\n",
    "            f.write(onx.SerializeToString())\n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {ONNX_PATH.resolve()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û–®–ò–ë–ö–ê —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ ONNX: {e}\")\n",
    "\n",
    "    # --- 5.3 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON-—Ñ–∞–π–ª–æ–≤ –¥–ª—è Backend ---\n",
    "    try:\n",
    "        with open(FEATURES_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"name\": \"fatigue_risk_v1\", # –ù–æ–≤–æ–µ –∏–º—è!\n",
    "                \"version\": \"1.0.0\",\n",
    "                \"features\": FEATURES\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ –ü–æ—Ä—è–¥–æ–∫ —Ñ–∏—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {FEATURES_JSON.resolve()}\")\n",
    "\n",
    "        with open(METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"best_model\": best_name,\n",
    "                **best_model_data[\"metrics\"]\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {METRICS_JSON.resolve()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û–®–ò–ë–ö–ê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON: {e}\")\n",
    "        \n",
    "    print(\"\\n--- –í–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω! ---\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n–û–®–ò–ë–ö–ê: 'models' –∏–ª–∏ 'FEATURES' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
